{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"10AsWgisclPd5_GGKRocHbcfQRW_n2zAE","timestamp":1680418458654},{"file_id":"1uZlCnKboSkoj43bmde4BIj-m-zc6tBPY","timestamp":1679805355619}],"machine_shape":"hm","collapsed_sections":["OB-MPB8Yz6t_","q5gku-vVA8hs","DIGhHufmBD4M"],"authorship_tag":"ABX9TyOxb/6SLyEWOWB83B1khrgR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["### Check Environment"],"metadata":{"id":"HGy9DuySzS6S"}},{"cell_type":"code","source":["!/opt/bin/nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxFfZ5VuNvfi","executionInfo":{"status":"ok","timestamp":1681609192055,"user_tz":300,"elapsed":16,"user":{"displayName":"David Tong","userId":"10645298735537320294"}},"outputId":"68d98e56-950b-4c15-db67-aa9af69d4994"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Apr 16 01:39:52 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["!python -V"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qJ2Yn8hhzWT3","executionInfo":{"status":"ok","timestamp":1681609196299,"user_tz":300,"elapsed":360,"user":{"displayName":"David Tong","userId":"10645298735537320294"}},"outputId":"8f8124ff-0f68-44e4-aecf-ae4d1aa1fd5b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.9.16\n"]}]},{"cell_type":"markdown","source":["### Import GitHub Project\n","https://github.com/huggingface/transformers"],"metadata":{"id":"d9xvUcmlNO5r"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dp6wYMqYcGwy","executionInfo":{"status":"ok","timestamp":1681609221963,"user_tz":300,"elapsed":18444,"user":{"displayName":"David Tong","userId":"10645298735537320294"}},"outputId":"70d02098-dff6-460b-d0e9-9ce28af21cbf"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Uy7djog8MlO8","executionInfo":{"status":"ok","timestamp":1681609272264,"user_tz":300,"elapsed":25077,"user":{"displayName":"David Tong","userId":"10645298735537320294"}}},"outputs":[],"source":["%%capture\n","\n","import os\n","from shutil import rmtree\n","\n","os.chdir('/content/drive/My Drive/git')\n","\n","GIT_PATH='/content/drive/My Drive/git/huggingface-transformers'\n","\n","FORCE_UPDATE=False\n","if FORCE_UPDATE:\n","  rmtree(GIT_PATH)\n","\n","if not os.path.exists(GIT_PATH):\n","  !git clone https://github.com/huggingface/transformers.git -b v4.27.0 huggingface-transformers"]},{"cell_type":"markdown","source":["### Import Modules"],"metadata":{"id":"rGxY8Rs_SUKy"}},{"cell_type":"code","source":["os.chdir('/content/drive/My Drive/git/huggingface-transformers')"],"metadata":{"id":"EpYGPAgcx_rX","executionInfo":{"status":"ok","timestamp":1681609282720,"user_tz":300,"elapsed":305,"user":{"displayName":"David Tong","userId":"10645298735537320294"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","!pip install transformers==4.27.0\n","!pip install -r examples/pytorch/question-answering/requirements.txt"],"metadata":{"id":"kOYIX2EwSPKT","executionInfo":{"status":"ok","timestamp":1681609306883,"user_tz":300,"elapsed":22730,"user":{"displayName":"David Tong","userId":"10645298735537320294"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"OB-MPB8Yz6t_"}},{"cell_type":"code","source":["os.chdir('/content/drive/My Drive/git/huggingface-transformers')"],"metadata":{"id":"OQ6p7H3b96AU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Baseline BERT\n","distilbert-base-uncased\n"],"metadata":{"id":"q5gku-vVA8hs"}},{"cell_type":"code","source":["!python examples/pytorch/question-answering/run_qa.py \\\n","            --model_name_or_path distilbert-base-uncased \\\n","            --dataset_name squad_v2 \\\n","            --do_train \\\n","            --do_eval \\\n","            --version_2_with_negative \\\n","            --learning_rate 3e-5 \\\n","            --num_train_epochs 2 \\\n","            --max_seq_length 384 \\\n","            --doc_stride 128 \\\n","            --per_device_eval_batch_size=16  \\\n","            --per_device_train_batch_size=16   \\\n","            --output_dir DISTILBERT_OUTPUT_DIR \\\n","            --overwrite_output_dir \\\n","            --save_steps 3000 \\"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WtxBPRWObQX-","executionInfo":{"status":"ok","timestamp":1680522344793,"user_tz":300,"elapsed":203953,"user":{"displayName":"David Tong","userId":"10645298735537320294"}},"outputId":"f9caaa2b-0cd2-4b68-e448-138b4cf18d24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-04-03 11:43:24.010235: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","04/03/2023 11:44:03 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/03/2023 11:44:03 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=3e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=OUTPUT_DIR/runs/Apr03_11-44-02_faa6a0985f5d,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=adamw_hf,\n","optim_args=None,\n","output_dir=OUTPUT_DIR,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=16,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=OUTPUT_DIR,\n","save_on_each_node=False,\n","save_steps=3000,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/03/2023 11:44:05 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/squad_v2/resolve/main/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp304g94sp\n","Downloading builder script: 100% 5.28k/5.28k [00:00<00:00, 4.36MB/s]\n","04/03/2023 11:44:06 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/squad_v2/resolve/main/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/e6f4860cc1c35cc48453e3ef385c5381c8a2453a71f4c8f115027046973b7adf.fc0184c3b7bb06ab8ab00fd19da286638a944c4ce8fbd5d62de27c39469e05a4.py\n","04/03/2023 11:44:06 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/e6f4860cc1c35cc48453e3ef385c5381c8a2453a71f4c8f115027046973b7adf.fc0184c3b7bb06ab8ab00fd19da286638a944c4ce8fbd5d62de27c39469e05a4.py\n","04/03/2023 11:44:07 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/squad_v2/resolve/main/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp94cfqhhh\n","Downloading metadata: 100% 2.40k/2.40k [00:00<00:00, 2.09MB/s]\n","04/03/2023 11:44:07 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/squad_v2/resolve/main/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/090eb8ef450f094e58a2fadd20eca7e0307da70f996d0090fdfb4d50b844da0c.311a7a0cc567f92fd461689d7aabb8bbd8c594e327ae5ec31c7166354ce7b709\n","04/03/2023 11:44:07 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/090eb8ef450f094e58a2fadd20eca7e0307da70f996d0090fdfb4d50b844da0c.311a7a0cc567f92fd461689d7aabb8bbd8c594e327ae5ec31c7166354ce7b709\n","04/03/2023 11:44:08 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/squad_v2/resolve/main/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp58tdhymc\n","Downloading readme: 100% 8.02k/8.02k [00:00<00:00, 6.71MB/s]\n","04/03/2023 11:44:09 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/squad_v2/resolve/main/README.md in cache at /root/.cache/huggingface/datasets/downloads/161a8eec71065c834b63dcd80453ac01a13f2fde78a03e36700f898eb4102917.917f13b853d68fb3212c0d6ef2d466ba6a48876fb9c9060087cc12e49f359536\n","04/03/2023 11:44:09 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/161a8eec71065c834b63dcd80453ac01a13f2fde78a03e36700f898eb4102917.917f13b853d68fb3212c0d6ef2d466ba6a48876fb9c9060087cc12e49f359536\n","04/03/2023 11:44:09 - INFO - datasets.builder - No config specified, defaulting to the single config: squad_v2/squad_v2\n","04/03/2023 11:44:09 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","04/03/2023 11:44:09 - INFO - datasets.builder - Generating dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","Downloading and preparing dataset squad_v2/squad_v2 to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n","04/03/2023 11:44:10 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]04/03/2023 11:44:11 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpacm8ja1t\n","\n","Downloading data:   0% 0.00/9.55M [00:00<?, ?B/s]\u001b[A\n","Downloading data:  85% 8.10M/9.55M [00:00<00:00, 81.0MB/s]\u001b[A\n","Downloading data: 19.7MB [00:00, 102MB/s]                 \u001b[A\n","Downloading data: 42.1MB [00:00, 111MB/s]\n","04/03/2023 11:44:12 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","04/03/2023 11:44:12 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","Downloading data files:  50% 1/2 [00:01<00:01,  1.58s/it]04/03/2023 11:44:12 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpobxod050\n","\n","Downloading data: 4.37MB [00:00, 72.1MB/s]      \n","04/03/2023 11:44:12 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","04/03/2023 11:44:12 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","Downloading data files: 100% 2/2 [00:02<00:00,  1.04s/it]\n","04/03/2023 11:44:12 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","04/03/2023 11:44:12 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 2327.58it/s]\n","04/03/2023 11:44:12 - INFO - datasets.builder - Generating train split\n","04/03/2023 11:44:22 - INFO - datasets.builder - Generating validation split\n","04/03/2023 11:44:23 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 99.88it/s]\n","Downloading (…)lve/main/config.json: 100% 483/483 [00:00<00:00, 71.6kB/s]\n","[INFO|configuration_utils.py:668] 2023-04-03 11:44:24,607 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n","[INFO|configuration_utils.py:720] 2023-04-03 11:44:26,942 >> Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.27.0\",\n","  \"vocab_size\": 30522\n","}\n","\n","Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 4.16kB/s]\n","[INFO|configuration_utils.py:668] 2023-04-03 11:44:28,489 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n","[INFO|configuration_utils.py:720] 2023-04-03 11:44:28,490 >> Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.27.0\",\n","  \"vocab_size\": 30522\n","}\n","\n","Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 419kB/s]\n","Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 504kB/s]\n","[INFO|tokenization_utils_base.py:1802] 2023-04-03 11:44:35,944 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/vocab.txt\n","[INFO|tokenization_utils_base.py:1802] 2023-04-03 11:44:35,944 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer.json\n","[INFO|tokenization_utils_base.py:1802] 2023-04-03 11:44:35,944 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1802] 2023-04-03 11:44:35,944 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1802] 2023-04-03 11:44:35,944 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer_config.json\n","[INFO|configuration_utils.py:668] 2023-04-03 11:44:35,944 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n","[INFO|configuration_utils.py:720] 2023-04-03 11:44:35,945 >> Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.27.0\",\n","  \"vocab_size\": 30522\n","}\n","\n","Downloading pytorch_model.bin: 100% 268M/268M [00:00<00:00, 360MB/s]\n","[INFO|modeling_utils.py:2403] 2023-04-03 11:45:11,398 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n","[INFO|modeling_distilbert.py:825] 2023-04-03 11:45:11,674 >> COME HERE YTONG !!!\n","[WARNING|modeling_utils.py:3022] 2023-04-03 11:45:12,400 >> Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3034] 2023-04-03 11:45:12,400 >> Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/20 [00:00<?, ? examples/s]04/03/2023 11:45:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-95ca0bcbaa426b3b.arrow\n","Running tokenizer on validation dataset:   0% 0/10 [00:00<?, ? examples/s]04/03/2023 11:45:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-18a9b2e0d727bdd2.arrow\n","Downloading builder script: 100% 6.47k/6.47k [00:00<00:00, 5.33MB/s]\n","Downloading extra modules: 100% 11.3k/11.3k [00:00<00:00, 9.16MB/s]\n","/content/drive/My Drive/git/transformers/src/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1740] 2023-04-03 11:45:20,651 >> ***** Running training *****\n","[INFO|trainer.py:1741] 2023-04-03 11:45:20,652 >>   Num examples = 20\n","[INFO|trainer.py:1742] 2023-04-03 11:45:20,652 >>   Num Epochs = 2\n","[INFO|trainer.py:1743] 2023-04-03 11:45:20,652 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1744] 2023-04-03 11:45:20,652 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1745] 2023-04-03 11:45:20,652 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1746] 2023-04-03 11:45:20,652 >>   Total optimization steps = 4\n","[INFO|trainer.py:1747] 2023-04-03 11:45:20,652 >>   Number of trainable parameters = 66364418\n","100% 4/4 [00:04<00:00,  1.49it/s][INFO|trainer.py:2012] 2023-04-03 11:45:25,793 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 5.1562, 'train_samples_per_second': 7.758, 'train_steps_per_second': 0.776, 'train_loss': 5.846332550048828, 'epoch': 2.0}\n","100% 4/4 [00:04<00:00,  1.03s/it]\n","[INFO|trainer.py:2814] 2023-04-03 11:45:25,812 >> Saving model checkpoint to OUTPUT_DIR\n","[INFO|configuration_utils.py:457] 2023-04-03 11:45:26,484 >> Configuration saved in OUTPUT_DIR/config.json\n","[INFO|modeling_utils.py:1762] 2023-04-03 11:45:33,306 >> Model weights saved in OUTPUT_DIR/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2163] 2023-04-03 11:45:34,028 >> tokenizer config file saved in OUTPUT_DIR/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2170] 2023-04-03 11:45:34,529 >> Special tokens file saved in OUTPUT_DIR/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     5.8463\n","  train_runtime            = 0:00:05.15\n","  train_samples            =         20\n","  train_samples_per_second =      7.758\n","  train_steps_per_second   =      0.776\n","04/03/2023 11:45:38 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:738] 2023-04-03 11:45:38,667 >> The following columns in the evaluation set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `DistilBertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3068] 2023-04-03 11:45:38,670 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3070] 2023-04-03 11:45:38,670 >>   Num examples = 10\n","[INFO|trainer.py:3073] 2023-04-03 11:45:38,670 >>   Batch size = 16\n","  0% 0/1 [00:00<?, ?it/s]04/03/2023 11:45:38 - INFO - utils_qa - Post-processing 10 example predictions split into 10 features.\n","\n","100% 10/10 [00:00<00:00, 433.71it/s]\n","04/03/2023 11:45:38 - INFO - utils_qa - Saving predictions to OUTPUT_DIR/eval_predictions.json.\n","04/03/2023 11:45:39 - INFO - utils_qa - Saving nbest_preds to OUTPUT_DIR/eval_nbest_predictions.json.\n","04/03/2023 11:45:40 - INFO - utils_qa - Saving null_odds to OUTPUT_DIR/eval_null_odds.json.\n","100% 1/1 [00:02<00:00,  2.28s/it]\n","***** eval metrics *****\n","  epoch                   =        2.0\n","  eval_HasAns_exact       =        0.0\n","  eval_HasAns_f1          =     4.7863\n","  eval_HasAns_total       =          6\n","  eval_NoAns_exact        =        0.0\n","  eval_NoAns_f1           =        0.0\n","  eval_NoAns_total        =          4\n","  eval_best_exact         =       40.0\n","  eval_best_exact_thresh  =        0.0\n","  eval_best_f1            =    41.5385\n","  eval_best_f1_thresh     =        0.0\n","  eval_exact              =        0.0\n","  eval_f1                 =     2.8718\n","  eval_runtime            = 0:00:00.16\n","  eval_samples            =         10\n","  eval_samples_per_second =     61.372\n","  eval_steps_per_second   =      6.137\n","  eval_total              =         10\n","[INFO|modelcard.py:451] 2023-04-03 11:45:42,631 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'config': 'squad_v2', 'split': 'validation', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"markdown","source":["### Improved Type 1 ALBERT\n","Classified Model\n","\n","albert-base-v2"],"metadata":{"id":"DIGhHufmBD4M"}},{"cell_type":"code","source":["OUTPUT_PATH = '/content/drive/My Drive/git/huggingface-transformers/ALBERT_OUTPUT_DIR'\n","\n","TRAIN_OR_RESUME = 'TRAIN' # change to RESUME if you want to resume from checkpoint\n","if TRAIN_OR_RESUME == 'TRAIN':\n","  rmtree(OUTPUT_PATH)"],"metadata":{"id":"Zx0KUQkI9oSl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if TRAIN_OR_RESUME == 'TRAIN':\n","  !python examples/pytorch/question-answering/run_qa.py \\\n","              --model_name_or_path albert-base-v2 \\\n","              --dataset_name squad_v2 \\\n","              --do_train \\\n","              --do_eval \\\n","              --version_2_with_negative \\\n","              --learning_rate 3e-5 \\\n","              --num_train_epochs 2 \\\n","              --max_seq_length 384 \\\n","              --doc_stride 128 \\\n","              --per_device_eval_batch_size 8  \\\n","              --per_device_train_batch_size 8   \\\n","              --output_dir ALBERT_OUTPUT_DIR \\\n","              --overwrite_output_dir \\\n","              --save_strategy \"steps\" \\\n","              --save_steps 3000\n","elif TRAIN_OR_RESUME == 'RESUME':\n","  !python examples/pytorch/question-answering/run_qa.py \\\n","              --model_name_or_path albert-base-v2 \\\n","              --dataset_name squad_v2 \\\n","              --do_train \\\n","              --do_eval \\\n","              --version_2_with_negative \\\n","              --learning_rate 3e-5 \\\n","              --num_train_epochs 2 \\\n","              --max_seq_length 384 \\\n","              --doc_stride 128 \\\n","              --per_device_eval_batch_size 8  \\\n","              --per_device_train_batch_size 8   \\\n","              --output_dir ALBERT_OUTPUT_DIR \\\n","              --save_strategy \"steps\" \\\n","              --save_steps 3000"],"metadata":{"id":"TKh9dOmZBQKs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Improved Type 1 SpanBERT\n","Classified Model\n","\n","SpanBERT/spanbert-large-cased"],"metadata":{"id":"RRa90ZOV8nni"}},{"cell_type":"code","source":["OUTPUT_PATH = '/content/drive/My Drive/git/huggingface-transformers/SPANBERT_OUTPUT_DIR'\n","\n","TRAIN_OR_RESUME = 'RESUME' # change to RESUME if you want to resume from checkpoint\n","if TRAIN_OR_RESUME == 'TRAIN':\n","  if os.path.exists(OUTPUT_PATH):\n","    rmtree(OUTPUT_PATH)"],"metadata":{"id":"9qf8liCw8ssj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if TRAIN_OR_RESUME == 'TRAIN':\n","  !python examples/pytorch/question-answering/run_qa.py \\\n","              --model_name_or_path SpanBERT/spanbert-large-cased \\\n","              --dataset_name squad_v2 \\\n","              --do_train \\\n","              --do_eval \\\n","              --version_2_with_negative \\\n","              --learning_rate 3e-5 \\\n","              --num_train_epochs 2 \\\n","              --max_seq_length 384 \\\n","              --doc_stride 128 \\\n","              --per_device_eval_batch_size 8  \\\n","              --per_device_train_batch_size 8   \\\n","              --output_dir SPANBERT_OUTPUT_DIR \\\n","              --overwrite_output_dir \\\n","              --save_strategy \"steps\" \\\n","              --save_steps 3000\n","elif TRAIN_OR_RESUME == 'RESUME':\n","  !python examples/pytorch/question-answering/run_qa.py \\\n","              --model_name_or_path SpanBERT/spanbert-large-cased \\\n","              --dataset_name squad_v2 \\\n","              --do_train \\\n","              --do_eval \\\n","              --version_2_with_negative \\\n","              --learning_rate 3e-5 \\\n","              --num_train_epochs 2 \\\n","              --max_seq_length 384 \\\n","              --doc_stride 128 \\\n","              --per_device_eval_batch_size 8  \\\n","              --per_device_train_batch_size 8   \\\n","              --output_dir SPANBERT_OUTPUT_DIR \\\n","              --save_strategy \"steps\" \\\n","              --save_steps 3000"],"metadata":{"id":"Zrfv5y4r8xwD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Improved Type I T5\n","Generatative Model"],"metadata":{"id":"gzRo0dTzZEuw"}},{"cell_type":"code","source":["OUTPUT_PATH = '/content/drive/My Drive/git/huggingface-transformers/T5_OUTPUT_DIR'\n","\n","TRAIN_OR_RESUME = 'TRAIN' # change to RESUME if you want to resume from checkpoint\n","if TRAIN_OR_RESUME == 'TRAIN':\n","  if os.path.exists(OUTPUT_PATH):\n","    rmtree(OUTPUT_PATH)"],"metadata":{"id":"mrIOd7HJukYr","executionInfo":{"status":"ok","timestamp":1681609623313,"user_tz":300,"elapsed":322,"user":{"displayName":"David Tong","userId":"10645298735537320294"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["if TRAIN_OR_RESUME == 'TRAIN':\n","  !python examples/pytorch/question-answering/run_seq2seq_qa.py \\\n","            --model_name_or_path t5-small \\\n","            --dataset_name squad_v2 \\\n","            --do_train \\\n","            --do_eval \\\n","            --context_column context \\\n","            --question_column question \\\n","            --answer_column answers \\\n","            --per_device_train_batch_size 8 \\\n","            --per_device_eval_batch_size 8  \\\n","            --learning_rate 3e-5 \\\n","            --num_train_epochs 2 \\\n","            --max_seq_length 384 \\\n","            --doc_stride 128 \\\n","            --output_dir T5_OUTPUT_DIR \\\n","            --overwrite_output_dir \\\n","            --save_strategy \"steps\" \\\n","            --save_steps 3000\n","elif TRAIN_OR_RESUME == 'RESUME':\n","  !python examples/pytorch/question-answering/run_seq2seq_qa.py \\\n","            --model_name_or_path t5-small \\\n","            --dataset_name squad_v2 \\\n","            --do_train \\\n","            --do_eval \\\n","            --context_column context \\\n","            --question_column question \\\n","            --answer_column answers \\\n","            --per_device_train_batch_size 8 \\\n","            --per_device_eval_batch_size 8  \\\n","            --learning_rate 3e-5 \\\n","            --num_train_epochs 2 \\\n","            --max_seq_length 384 \\\n","            --doc_stride 128 \\\n","            --output_dir T5_OUTPUT_DIR \\\n","            --save_strategy \"steps\" \\\n","            --save_steps 3000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"suXqIot7urOa","executionInfo":{"status":"ok","timestamp":1681616488183,"user_tz":300,"elapsed":6778963,"user":{"displayName":"David Tong","userId":"10645298735537320294"}},"outputId":"1007c714-4713-432f-bf06-aa841e69a53a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-04-16 01:48:33.347618: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","04/16/2023 01:48:35 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/16/2023 01:48:35 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=3e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=T5_OUTPUT_DIR/runs/Apr16_01-48-35_583d1c8df914,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=adamw_hf,\n","optim_args=None,\n","output_dir=T5_OUTPUT_DIR,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=8,\n","predict_with_generate=False,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=T5_OUTPUT_DIR,\n","save_on_each_node=False,\n","save_steps=3000,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/16/2023 01:48:37 - INFO - datasets.builder - No config specified, defaulting to the single config: squad_v2/squad_v2\n","04/16/2023 01:48:37 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","04/16/2023 01:48:37 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","04/16/2023 01:48:37 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","04/16/2023 01:48:37 - WARNING - datasets.builder - Found cached dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","04/16/2023 01:48:37 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","100% 2/2 [00:00<00:00, 407.06it/s]\n","Downloading (…)lve/main/config.json: 100% 1.21k/1.21k [00:00<00:00, 197kB/s]\n","[INFO|configuration_utils.py:668] 2023-04-16 01:48:37,779 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/5bf53e1f76b1430d9302d735c613c5f5677e32a6/config.json\n","[INFO|configuration_utils.py:720] 2023-04-16 01:48:37,782 >> Model config T5Config {\n","  \"_name_or_path\": \"t5-small\",\n","  \"architectures\": [\n","    \"T5ForConditionalGeneration\"\n","  ],\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 512,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"relu\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"relu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": false,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_decoder_layers\": 6,\n","  \"num_heads\": 8,\n","  \"num_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"transformers_version\": \"4.27.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n","[INFO|tokenization_auto.py:479] 2023-04-16 01:48:38,020 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:668] 2023-04-16 01:48:38,262 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/5bf53e1f76b1430d9302d735c613c5f5677e32a6/config.json\n","[INFO|configuration_utils.py:720] 2023-04-16 01:48:38,263 >> Model config T5Config {\n","  \"_name_or_path\": \"t5-small\",\n","  \"architectures\": [\n","    \"T5ForConditionalGeneration\"\n","  ],\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 512,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"relu\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"relu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": false,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_decoder_layers\": 6,\n","  \"num_heads\": 8,\n","  \"num_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"transformers_version\": \"4.27.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n","Downloading (…)ve/main/spiece.model: 100% 792k/792k [00:00<00:00, 59.4MB/s]\n","Downloading (…)/main/tokenizer.json: 100% 1.39M/1.39M [00:00<00:00, 6.58MB/s]\n","[INFO|tokenization_utils_base.py:1802] 2023-04-16 01:48:40,250 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/5bf53e1f76b1430d9302d735c613c5f5677e32a6/spiece.model\n","[INFO|tokenization_utils_base.py:1802] 2023-04-16 01:48:40,250 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/5bf53e1f76b1430d9302d735c613c5f5677e32a6/tokenizer.json\n","[INFO|tokenization_utils_base.py:1802] 2023-04-16 01:48:40,251 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1802] 2023-04-16 01:48:40,251 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1802] 2023-04-16 01:48:40,251 >> loading file tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:668] 2023-04-16 01:48:40,251 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/5bf53e1f76b1430d9302d735c613c5f5677e32a6/config.json\n","[INFO|configuration_utils.py:720] 2023-04-16 01:48:40,252 >> Model config T5Config {\n","  \"_name_or_path\": \"t5-small\",\n","  \"architectures\": [\n","    \"T5ForConditionalGeneration\"\n","  ],\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 512,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"relu\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"relu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": false,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_decoder_layers\": 6,\n","  \"num_heads\": 8,\n","  \"num_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"transformers_version\": \"4.27.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n","/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n","Downloading pytorch_model.bin: 100% 242M/242M [00:00<00:00, 258MB/s]\n","[INFO|modeling_utils.py:2403] 2023-04-16 01:48:41,529 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/5bf53e1f76b1430d9302d735c613c5f5677e32a6/pytorch_model.bin\n","[INFO|configuration_utils.py:575] 2023-04-16 01:48:41,682 >> Generate config GenerationConfig {\n","  \"_from_model_config\": true,\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0,\n","  \"transformers_version\": \"4.27.0\"\n","}\n","\n","[INFO|modeling_utils.py:3032] 2023-04-16 01:48:42,533 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:3040] 2023-04-16 01:48:42,533 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n","Downloading (…)neration_config.json: 100% 147/147 [00:00<00:00, 24.2kB/s]\n","[INFO|configuration_utils.py:537] 2023-04-16 01:48:43,023 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/5bf53e1f76b1430d9302d735c613c5f5677e32a6/generation_config.json\n","[INFO|configuration_utils.py:575] 2023-04-16 01:48:43,023 >> Generate config GenerationConfig {\n","  \"_from_model_config\": true,\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0,\n","  \"transformers_version\": \"4.27.0\"\n","}\n","\n","Running tokenizer on train dataset:   0% 0/130319 [00:00<?, ? examples/s]04/16/2023 01:48:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-ac6126e5c4b38f1a.arrow\n","Running tokenizer on validation dataset:   0% 0/11873 [00:00<?, ? examples/s]04/16/2023 01:49:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-b846e93e79de9679.arrow\n","Downloading builder script: 100% 4.53k/4.53k [00:00<00:00, 3.73MB/s]\n","Downloading extra modules: 100% 3.32k/3.32k [00:00<00:00, 3.13MB/s]\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1740] 2023-04-16 01:49:32,007 >> ***** Running training *****\n","[INFO|trainer.py:1741] 2023-04-16 01:49:32,007 >>   Num examples = 130319\n","[INFO|trainer.py:1742] 2023-04-16 01:49:32,007 >>   Num Epochs = 2\n","[INFO|trainer.py:1743] 2023-04-16 01:49:32,007 >>   Instantaneous batch size per device = 8\n","[INFO|trainer.py:1744] 2023-04-16 01:49:32,007 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:1745] 2023-04-16 01:49:32,007 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1746] 2023-04-16 01:49:32,007 >>   Total optimization steps = 32580\n","[INFO|trainer.py:1747] 2023-04-16 01:49:32,008 >>   Number of trainable parameters = 60506624\n","  0% 0/32580 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-04-16 01:49:32,045 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","{'loss': 0.7416, 'learning_rate': 2.9539594843462247e-05, 'epoch': 0.03}\n","{'loss': 0.6432, 'learning_rate': 2.9079189686924494e-05, 'epoch': 0.06}\n","{'loss': 0.6327, 'learning_rate': 2.861878453038674e-05, 'epoch': 0.09}\n","{'loss': 0.5885, 'learning_rate': 2.815837937384899e-05, 'epoch': 0.12}\n","{'loss': 0.6052, 'learning_rate': 2.7697974217311233e-05, 'epoch': 0.15}\n","{'loss': 0.5948, 'learning_rate': 2.7237569060773482e-05, 'epoch': 0.18}\n","  9% 3000/32580 [09:56<1:40:00,  4.93it/s][INFO|trainer.py:2814] 2023-04-16 01:59:28,772 >> Saving model checkpoint to T5_OUTPUT_DIR/checkpoint-3000\n","[INFO|configuration_utils.py:457] 2023-04-16 01:59:28,776 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:362] 2023-04-16 01:59:28,781 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:1762] 2023-04-16 01:59:29,344 >> Model weights saved in T5_OUTPUT_DIR/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2163] 2023-04-16 01:59:29,348 >> tokenizer config file saved in T5_OUTPUT_DIR/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2170] 2023-04-16 01:59:29,350 >> Special tokens file saved in T5_OUTPUT_DIR/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.5816, 'learning_rate': 2.677716390423573e-05, 'epoch': 0.21}\n","{'loss': 0.6103, 'learning_rate': 2.6316758747697975e-05, 'epoch': 0.25}\n","{'loss': 0.5771, 'learning_rate': 2.585635359116022e-05, 'epoch': 0.28}\n","{'loss': 0.5684, 'learning_rate': 2.5395948434622468e-05, 'epoch': 0.31}\n","{'loss': 0.5696, 'learning_rate': 2.4935543278084714e-05, 'epoch': 0.34}\n","{'loss': 0.5947, 'learning_rate': 2.4475138121546964e-05, 'epoch': 0.37}\n"," 18% 6000/32580 [20:06<1:30:06,  4.92it/s][INFO|trainer.py:2814] 2023-04-16 02:09:38,710 >> Saving model checkpoint to T5_OUTPUT_DIR/checkpoint-6000\n","[INFO|configuration_utils.py:457] 2023-04-16 02:09:38,714 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-6000/config.json\n","[INFO|configuration_utils.py:362] 2023-04-16 02:09:38,719 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-6000/generation_config.json\n","[INFO|modeling_utils.py:1762] 2023-04-16 02:09:39,272 >> Model weights saved in T5_OUTPUT_DIR/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2163] 2023-04-16 02:09:39,276 >> tokenizer config file saved in T5_OUTPUT_DIR/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2170] 2023-04-16 02:09:39,279 >> Special tokens file saved in T5_OUTPUT_DIR/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.5876, 'learning_rate': 2.4014732965009207e-05, 'epoch': 0.4}\n","{'loss': 0.5881, 'learning_rate': 2.3554327808471457e-05, 'epoch': 0.43}\n","{'loss': 0.5641, 'learning_rate': 2.3093922651933703e-05, 'epoch': 0.46}\n","{'loss': 0.5484, 'learning_rate': 2.2633517495395946e-05, 'epoch': 0.49}\n","{'loss': 0.5749, 'learning_rate': 2.2173112338858196e-05, 'epoch': 0.52}\n","{'loss': 0.5492, 'learning_rate': 2.1712707182320442e-05, 'epoch': 0.55}\n"," 28% 9000/32580 [30:15<1:18:57,  4.98it/s][INFO|trainer.py:2814] 2023-04-16 02:19:47,790 >> Saving model checkpoint to T5_OUTPUT_DIR/checkpoint-9000\n","[INFO|configuration_utils.py:457] 2023-04-16 02:19:47,794 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-9000/config.json\n","[INFO|configuration_utils.py:362] 2023-04-16 02:19:47,799 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-9000/generation_config.json\n","[INFO|modeling_utils.py:1762] 2023-04-16 02:19:48,350 >> Model weights saved in T5_OUTPUT_DIR/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2163] 2023-04-16 02:19:48,355 >> tokenizer config file saved in T5_OUTPUT_DIR/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2170] 2023-04-16 02:19:48,359 >> Special tokens file saved in T5_OUTPUT_DIR/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.5589, 'learning_rate': 2.125230202578269e-05, 'epoch': 0.58}\n","{'loss': 0.5493, 'learning_rate': 2.0791896869244935e-05, 'epoch': 0.61}\n","{'loss': 0.5803, 'learning_rate': 2.0331491712707185e-05, 'epoch': 0.64}\n","{'loss': 0.5339, 'learning_rate': 1.9871086556169428e-05, 'epoch': 0.68}\n","{'loss': 0.559, 'learning_rate': 1.9410681399631678e-05, 'epoch': 0.71}\n","{'loss': 0.5368, 'learning_rate': 1.8950276243093924e-05, 'epoch': 0.74}\n"," 37% 12000/32580 [40:25<1:09:30,  4.94it/s][INFO|trainer.py:2814] 2023-04-16 02:29:57,879 >> Saving model checkpoint to T5_OUTPUT_DIR/checkpoint-12000\n","[INFO|configuration_utils.py:457] 2023-04-16 02:29:57,883 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-12000/config.json\n","[INFO|configuration_utils.py:362] 2023-04-16 02:29:57,888 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-12000/generation_config.json\n","[INFO|modeling_utils.py:1762] 2023-04-16 02:29:58,418 >> Model weights saved in T5_OUTPUT_DIR/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2163] 2023-04-16 02:29:58,421 >> tokenizer config file saved in T5_OUTPUT_DIR/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2170] 2023-04-16 02:29:58,424 >> Special tokens file saved in T5_OUTPUT_DIR/checkpoint-12000/special_tokens_map.json\n","{'loss': 0.5391, 'learning_rate': 1.848987108655617e-05, 'epoch': 0.77}\n","{'loss': 0.5501, 'learning_rate': 1.8029465930018417e-05, 'epoch': 0.8}\n","{'loss': 0.5555, 'learning_rate': 1.7569060773480663e-05, 'epoch': 0.83}\n","{'loss': 0.5435, 'learning_rate': 1.710865561694291e-05, 'epoch': 0.86}\n","{'loss': 0.5487, 'learning_rate': 1.664825046040516e-05, 'epoch': 0.89}\n","{'loss': 0.5099, 'learning_rate': 1.6187845303867402e-05, 'epoch': 0.92}\n"," 46% 15000/32580 [50:36<59:26,  4.93it/s][INFO|trainer.py:2814] 2023-04-16 02:40:08,677 >> Saving model checkpoint to T5_OUTPUT_DIR/checkpoint-15000\n","[INFO|configuration_utils.py:457] 2023-04-16 02:40:08,681 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-15000/config.json\n","[INFO|configuration_utils.py:362] 2023-04-16 02:40:08,685 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-15000/generation_config.json\n","[INFO|modeling_utils.py:1762] 2023-04-16 02:40:09,212 >> Model weights saved in T5_OUTPUT_DIR/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2163] 2023-04-16 02:40:09,216 >> tokenizer config file saved in T5_OUTPUT_DIR/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2170] 2023-04-16 02:40:09,219 >> Special tokens file saved in T5_OUTPUT_DIR/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.5427, 'learning_rate': 1.572744014732965e-05, 'epoch': 0.95}\n","{'loss': 0.5372, 'learning_rate': 1.52670349907919e-05, 'epoch': 0.98}\n","{'loss': 0.5156, 'learning_rate': 1.4806629834254145e-05, 'epoch': 1.01}\n","{'loss': 0.498, 'learning_rate': 1.4346224677716391e-05, 'epoch': 1.04}\n","{'loss': 0.5142, 'learning_rate': 1.3885819521178638e-05, 'epoch': 1.07}\n","{'loss': 0.4931, 'learning_rate': 1.3425414364640886e-05, 'epoch': 1.1}\n"," 55% 18000/32580 [1:00:47<49:20,  4.93it/s][INFO|trainer.py:2814] 2023-04-16 02:50:19,628 >> Saving model checkpoint to T5_OUTPUT_DIR/checkpoint-18000\n","[INFO|configuration_utils.py:457] 2023-04-16 02:50:19,632 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-18000/config.json\n","[INFO|configuration_utils.py:362] 2023-04-16 02:50:19,636 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-18000/generation_config.json\n","[INFO|modeling_utils.py:1762] 2023-04-16 02:50:20,161 >> Model weights saved in T5_OUTPUT_DIR/checkpoint-18000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2163] 2023-04-16 02:50:20,164 >> tokenizer config file saved in T5_OUTPUT_DIR/checkpoint-18000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2170] 2023-04-16 02:50:20,167 >> Special tokens file saved in T5_OUTPUT_DIR/checkpoint-18000/special_tokens_map.json\n","{'loss': 0.494, 'learning_rate': 1.2965009208103132e-05, 'epoch': 1.14}\n","{'loss': 0.5059, 'learning_rate': 1.2504604051565377e-05, 'epoch': 1.17}\n","{'loss': 0.4906, 'learning_rate': 1.2044198895027623e-05, 'epoch': 1.2}\n","{'loss': 0.5016, 'learning_rate': 1.1583793738489871e-05, 'epoch': 1.23}\n","{'loss': 0.5176, 'learning_rate': 1.1123388581952118e-05, 'epoch': 1.26}\n","{'loss': 0.4973, 'learning_rate': 1.0662983425414364e-05, 'epoch': 1.29}\n"," 64% 21000/32580 [1:10:58<39:11,  4.92it/s][INFO|trainer.py:2814] 2023-04-16 03:00:30,954 >> Saving model checkpoint to T5_OUTPUT_DIR/checkpoint-21000\n","[INFO|configuration_utils.py:457] 2023-04-16 03:00:30,959 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-21000/config.json\n","[INFO|configuration_utils.py:362] 2023-04-16 03:00:30,964 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-21000/generation_config.json\n","[INFO|modeling_utils.py:1762] 2023-04-16 03:00:31,498 >> Model weights saved in T5_OUTPUT_DIR/checkpoint-21000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2163] 2023-04-16 03:00:31,502 >> tokenizer config file saved in T5_OUTPUT_DIR/checkpoint-21000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2170] 2023-04-16 03:00:31,505 >> Special tokens file saved in T5_OUTPUT_DIR/checkpoint-21000/special_tokens_map.json\n","{'loss': 0.5262, 'learning_rate': 1.0202578268876612e-05, 'epoch': 1.32}\n","{'loss': 0.4775, 'learning_rate': 9.742173112338858e-06, 'epoch': 1.35}\n","{'loss': 0.5027, 'learning_rate': 9.281767955801105e-06, 'epoch': 1.38}\n","{'loss': 0.4993, 'learning_rate': 8.821362799263353e-06, 'epoch': 1.41}\n","{'loss': 0.5077, 'learning_rate': 8.3609576427256e-06, 'epoch': 1.44}\n","{'loss': 0.5154, 'learning_rate': 7.900552486187846e-06, 'epoch': 1.47}\n"," 74% 24000/32580 [1:21:08<28:56,  4.94it/s][INFO|trainer.py:2814] 2023-04-16 03:10:40,697 >> Saving model checkpoint to T5_OUTPUT_DIR/checkpoint-24000\n","[INFO|configuration_utils.py:457] 2023-04-16 03:10:40,700 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-24000/config.json\n","[INFO|configuration_utils.py:362] 2023-04-16 03:10:40,705 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-24000/generation_config.json\n","[INFO|modeling_utils.py:1762] 2023-04-16 03:10:41,243 >> Model weights saved in T5_OUTPUT_DIR/checkpoint-24000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2163] 2023-04-16 03:10:41,247 >> tokenizer config file saved in T5_OUTPUT_DIR/checkpoint-24000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2170] 2023-04-16 03:10:41,250 >> Special tokens file saved in T5_OUTPUT_DIR/checkpoint-24000/special_tokens_map.json\n","{'loss': 0.5033, 'learning_rate': 7.440147329650092e-06, 'epoch': 1.5}\n","{'loss': 0.4877, 'learning_rate': 6.9797421731123384e-06, 'epoch': 1.53}\n","{'loss': 0.51, 'learning_rate': 6.519337016574586e-06, 'epoch': 1.57}\n","{'loss': 0.4953, 'learning_rate': 6.058931860036833e-06, 'epoch': 1.6}\n","{'loss': 0.4974, 'learning_rate': 5.598526703499079e-06, 'epoch': 1.63}\n","{'loss': 0.5005, 'learning_rate': 5.1381215469613265e-06, 'epoch': 1.66}\n"," 83% 27000/32580 [1:31:18<18:54,  4.92it/s][INFO|trainer.py:2814] 2023-04-16 03:20:50,472 >> Saving model checkpoint to T5_OUTPUT_DIR/checkpoint-27000\n","[INFO|configuration_utils.py:457] 2023-04-16 03:20:50,476 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-27000/config.json\n","[INFO|configuration_utils.py:362] 2023-04-16 03:20:50,480 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-27000/generation_config.json\n","[INFO|modeling_utils.py:1762] 2023-04-16 03:20:51,007 >> Model weights saved in T5_OUTPUT_DIR/checkpoint-27000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2163] 2023-04-16 03:20:51,011 >> tokenizer config file saved in T5_OUTPUT_DIR/checkpoint-27000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2170] 2023-04-16 03:20:51,013 >> Special tokens file saved in T5_OUTPUT_DIR/checkpoint-27000/special_tokens_map.json\n","{'loss': 0.4813, 'learning_rate': 4.677716390423573e-06, 'epoch': 1.69}\n","{'loss': 0.4896, 'learning_rate': 4.21731123388582e-06, 'epoch': 1.72}\n","{'loss': 0.5023, 'learning_rate': 3.756906077348067e-06, 'epoch': 1.75}\n","{'loss': 0.4847, 'learning_rate': 3.296500920810313e-06, 'epoch': 1.78}\n","{'loss': 0.5052, 'learning_rate': 2.8360957642725597e-06, 'epoch': 1.81}\n","{'loss': 0.4756, 'learning_rate': 2.375690607734807e-06, 'epoch': 1.84}\n"," 92% 30000/32580 [1:41:28<08:44,  4.92it/s][INFO|trainer.py:2814] 2023-04-16 03:31:00,219 >> Saving model checkpoint to T5_OUTPUT_DIR/checkpoint-30000\n","[INFO|configuration_utils.py:457] 2023-04-16 03:31:00,223 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-30000/config.json\n","[INFO|configuration_utils.py:362] 2023-04-16 03:31:00,228 >> Configuration saved in T5_OUTPUT_DIR/checkpoint-30000/generation_config.json\n","[INFO|modeling_utils.py:1762] 2023-04-16 03:31:00,755 >> Model weights saved in T5_OUTPUT_DIR/checkpoint-30000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2163] 2023-04-16 03:31:00,758 >> tokenizer config file saved in T5_OUTPUT_DIR/checkpoint-30000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2170] 2023-04-16 03:31:00,761 >> Special tokens file saved in T5_OUTPUT_DIR/checkpoint-30000/special_tokens_map.json\n","{'loss': 0.4786, 'learning_rate': 1.9152854511970537e-06, 'epoch': 1.87}\n","{'loss': 0.5219, 'learning_rate': 1.4548802946593003e-06, 'epoch': 1.9}\n","{'loss': 0.4807, 'learning_rate': 9.944751381215469e-07, 'epoch': 1.93}\n","{'loss': 0.4894, 'learning_rate': 5.340699815837937e-07, 'epoch': 1.96}\n","{'loss': 0.4699, 'learning_rate': 7.366482504604051e-08, 'epoch': 2.0}\n","100% 32580/32580 [1:50:12<00:00,  5.04it/s][INFO|trainer.py:2012] 2023-04-16 03:39:44,763 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 6612.7653, 'train_samples_per_second': 39.414, 'train_steps_per_second': 4.927, 'train_loss': 0.5351406904142133, 'epoch': 2.0}\n","100% 32580/32580 [1:50:12<00:00,  4.93it/s]\n","[INFO|trainer.py:2814] 2023-04-16 03:39:44,778 >> Saving model checkpoint to T5_OUTPUT_DIR\n","[INFO|configuration_utils.py:457] 2023-04-16 03:39:44,783 >> Configuration saved in T5_OUTPUT_DIR/config.json\n","[INFO|configuration_utils.py:362] 2023-04-16 03:39:44,787 >> Configuration saved in T5_OUTPUT_DIR/generation_config.json\n","[INFO|modeling_utils.py:1762] 2023-04-16 03:39:45,327 >> Model weights saved in T5_OUTPUT_DIR/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2163] 2023-04-16 03:39:45,330 >> tokenizer config file saved in T5_OUTPUT_DIR/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2170] 2023-04-16 03:39:45,333 >> Special tokens file saved in T5_OUTPUT_DIR/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     0.5351\n","  train_runtime            = 1:50:12.76\n","  train_samples            =     130319\n","  train_samples_per_second =     39.414\n","  train_steps_per_second   =      4.927\n","04/16/2023 03:39:45 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:738] 2023-04-16 03:39:45,370 >> The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3068] 2023-04-16 03:39:45,372 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3070] 2023-04-16 03:39:45,372 >>   Num examples = 12309\n","[INFO|trainer.py:3073] 2023-04-16 03:39:45,372 >>   Batch size = 8\n","100% 1539/1539 [01:41<00:00, 15.14it/s]\n","***** eval metrics *****\n","  epoch                   =        2.0\n","  eval_loss               =       0.74\n","  eval_runtime            = 0:01:41.66\n","  eval_samples            =      12309\n","  eval_samples_per_second =    121.074\n","  eval_steps_per_second   =     15.138\n"]}]}]}