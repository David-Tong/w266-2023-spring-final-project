{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"10AsWgisclPd5_GGKRocHbcfQRW_n2zAE","timestamp":1680418458654},{"file_id":"1uZlCnKboSkoj43bmde4BIj-m-zc6tBPY","timestamp":1679805355619}],"machine_shape":"hm","collapsed_sections":["q5gku-vVA8hs","RRa90ZOV8nni"],"authorship_tag":"ABX9TyPmLbdxL+UR+vZNWWVR6SWO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["### Check Environment"],"metadata":{"id":"HGy9DuySzS6S"}},{"cell_type":"code","source":["!/opt/bin/nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxFfZ5VuNvfi","executionInfo":{"status":"ok","timestamp":1680686326056,"user_tz":300,"elapsed":340,"user":{"displayName":"David Tong","userId":"10645298735537320294"}},"outputId":"3c2a68f6-0c6c-4a0a-b4c5-212539a989ff"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Apr  5 09:18:47 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["!python -V"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qJ2Yn8hhzWT3","executionInfo":{"status":"ok","timestamp":1680686328587,"user_tz":300,"elapsed":8,"user":{"displayName":"David Tong","userId":"10645298735537320294"}},"outputId":"479015cd-26a6-42d8-908f-8b0ebf34e15f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.9.16\n"]}]},{"cell_type":"markdown","source":["### Import GitHub Project\n","https://github.com/huggingface/transformers"],"metadata":{"id":"d9xvUcmlNO5r"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dp6wYMqYcGwy","executionInfo":{"status":"ok","timestamp":1680686356718,"user_tz":300,"elapsed":22002,"user":{"displayName":"David Tong","userId":"10645298735537320294"}},"outputId":"702ed3c6-f921-4f49-fe1f-b7fa8da1e4f3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Uy7djog8MlO8","executionInfo":{"status":"ok","timestamp":1680686358419,"user_tz":300,"elapsed":1708,"user":{"displayName":"David Tong","userId":"10645298735537320294"}}},"outputs":[],"source":["%%capture\n","\n","import os\n","from shutil import rmtree\n","\n","os.chdir('/content/drive/My Drive/git')\n","\n","GIT_PATH='/content/drive/My Drive/git/transformers'\n","\n","FORCE_UPDATE=False\n","if FORCE_UPDATE:\n","  rmtree(GIT_PATH)\n","\n","if not os.path.exists(GIT_PATH):\n","  !git clone https://github.com/huggingface/transformers.git -b v4.27.0"]},{"cell_type":"markdown","source":["### Import Modules"],"metadata":{"id":"rGxY8Rs_SUKy"}},{"cell_type":"code","source":["os.chdir('/content/drive/My Drive/git/transformers')"],"metadata":{"id":"EpYGPAgcx_rX","executionInfo":{"status":"ok","timestamp":1680686358420,"user_tz":300,"elapsed":6,"user":{"displayName":"David Tong","userId":"10645298735537320294"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","!pip install transformers==4.27.0\n","!pip install -r examples/pytorch/question-answering/requirements.txt"],"metadata":{"id":"kOYIX2EwSPKT","executionInfo":{"status":"ok","timestamp":1680686389913,"user_tz":300,"elapsed":31497,"user":{"displayName":"David Tong","userId":"10645298735537320294"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"OB-MPB8Yz6t_"}},{"cell_type":"code","source":["os.chdir('/content/drive/My Drive/git/transformers')"],"metadata":{"id":"OQ6p7H3b96AU","executionInfo":{"status":"ok","timestamp":1680686389914,"user_tz":300,"elapsed":8,"user":{"displayName":"David Tong","userId":"10645298735537320294"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Baseline BERT\n","distilbert-base-uncased\n"],"metadata":{"id":"q5gku-vVA8hs"}},{"cell_type":"code","source":["!python examples/pytorch/question-answering/run_qa.py \\\n","            --model_name_or_path distilbert-base-uncased \\\n","            --dataset_name squad_v2 \\\n","            --do_train \\\n","            --do_eval \\\n","            --version_2_with_negative \\\n","            --learning_rate 3e-5 \\\n","            --num_train_epochs 2 \\\n","            --max_seq_length 384 \\\n","            --doc_stride 128 \\\n","            --per_device_eval_batch_size=16  \\\n","            --per_device_train_batch_size=16   \\\n","            --output_dir DISTILBERT_OUTPUT_DIR \\\n","            --overwrite_output_dir \\\n","            --save_steps 3000 \\"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WtxBPRWObQX-","executionInfo":{"status":"ok","timestamp":1680522344793,"user_tz":300,"elapsed":203953,"user":{"displayName":"David Tong","userId":"10645298735537320294"}},"outputId":"f9caaa2b-0cd2-4b68-e448-138b4cf18d24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-04-03 11:43:24.010235: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","04/03/2023 11:44:03 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/03/2023 11:44:03 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=3e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=OUTPUT_DIR/runs/Apr03_11-44-02_faa6a0985f5d,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=adamw_hf,\n","optim_args=None,\n","output_dir=OUTPUT_DIR,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=16,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=OUTPUT_DIR,\n","save_on_each_node=False,\n","save_steps=3000,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/03/2023 11:44:05 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/squad_v2/resolve/main/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp304g94sp\n","Downloading builder script: 100% 5.28k/5.28k [00:00<00:00, 4.36MB/s]\n","04/03/2023 11:44:06 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/squad_v2/resolve/main/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/e6f4860cc1c35cc48453e3ef385c5381c8a2453a71f4c8f115027046973b7adf.fc0184c3b7bb06ab8ab00fd19da286638a944c4ce8fbd5d62de27c39469e05a4.py\n","04/03/2023 11:44:06 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/e6f4860cc1c35cc48453e3ef385c5381c8a2453a71f4c8f115027046973b7adf.fc0184c3b7bb06ab8ab00fd19da286638a944c4ce8fbd5d62de27c39469e05a4.py\n","04/03/2023 11:44:07 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/squad_v2/resolve/main/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp94cfqhhh\n","Downloading metadata: 100% 2.40k/2.40k [00:00<00:00, 2.09MB/s]\n","04/03/2023 11:44:07 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/squad_v2/resolve/main/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/090eb8ef450f094e58a2fadd20eca7e0307da70f996d0090fdfb4d50b844da0c.311a7a0cc567f92fd461689d7aabb8bbd8c594e327ae5ec31c7166354ce7b709\n","04/03/2023 11:44:07 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/090eb8ef450f094e58a2fadd20eca7e0307da70f996d0090fdfb4d50b844da0c.311a7a0cc567f92fd461689d7aabb8bbd8c594e327ae5ec31c7166354ce7b709\n","04/03/2023 11:44:08 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/squad_v2/resolve/main/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp58tdhymc\n","Downloading readme: 100% 8.02k/8.02k [00:00<00:00, 6.71MB/s]\n","04/03/2023 11:44:09 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/squad_v2/resolve/main/README.md in cache at /root/.cache/huggingface/datasets/downloads/161a8eec71065c834b63dcd80453ac01a13f2fde78a03e36700f898eb4102917.917f13b853d68fb3212c0d6ef2d466ba6a48876fb9c9060087cc12e49f359536\n","04/03/2023 11:44:09 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/161a8eec71065c834b63dcd80453ac01a13f2fde78a03e36700f898eb4102917.917f13b853d68fb3212c0d6ef2d466ba6a48876fb9c9060087cc12e49f359536\n","04/03/2023 11:44:09 - INFO - datasets.builder - No config specified, defaulting to the single config: squad_v2/squad_v2\n","04/03/2023 11:44:09 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","04/03/2023 11:44:09 - INFO - datasets.builder - Generating dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","Downloading and preparing dataset squad_v2/squad_v2 to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n","04/03/2023 11:44:10 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]04/03/2023 11:44:11 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpacm8ja1t\n","\n","Downloading data:   0% 0.00/9.55M [00:00<?, ?B/s]\u001b[A\n","Downloading data:  85% 8.10M/9.55M [00:00<00:00, 81.0MB/s]\u001b[A\n","Downloading data: 19.7MB [00:00, 102MB/s]                 \u001b[A\n","Downloading data: 42.1MB [00:00, 111MB/s]\n","04/03/2023 11:44:12 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","04/03/2023 11:44:12 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","Downloading data files:  50% 1/2 [00:01<00:01,  1.58s/it]04/03/2023 11:44:12 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpobxod050\n","\n","Downloading data: 4.37MB [00:00, 72.1MB/s]      \n","04/03/2023 11:44:12 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","04/03/2023 11:44:12 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","Downloading data files: 100% 2/2 [00:02<00:00,  1.04s/it]\n","04/03/2023 11:44:12 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","04/03/2023 11:44:12 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 2327.58it/s]\n","04/03/2023 11:44:12 - INFO - datasets.builder - Generating train split\n","04/03/2023 11:44:22 - INFO - datasets.builder - Generating validation split\n","04/03/2023 11:44:23 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 99.88it/s]\n","Downloading (…)lve/main/config.json: 100% 483/483 [00:00<00:00, 71.6kB/s]\n","[INFO|configuration_utils.py:668] 2023-04-03 11:44:24,607 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n","[INFO|configuration_utils.py:720] 2023-04-03 11:44:26,942 >> Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.27.0\",\n","  \"vocab_size\": 30522\n","}\n","\n","Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 4.16kB/s]\n","[INFO|configuration_utils.py:668] 2023-04-03 11:44:28,489 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n","[INFO|configuration_utils.py:720] 2023-04-03 11:44:28,490 >> Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.27.0\",\n","  \"vocab_size\": 30522\n","}\n","\n","Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 419kB/s]\n","Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 504kB/s]\n","[INFO|tokenization_utils_base.py:1802] 2023-04-03 11:44:35,944 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/vocab.txt\n","[INFO|tokenization_utils_base.py:1802] 2023-04-03 11:44:35,944 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer.json\n","[INFO|tokenization_utils_base.py:1802] 2023-04-03 11:44:35,944 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1802] 2023-04-03 11:44:35,944 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1802] 2023-04-03 11:44:35,944 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer_config.json\n","[INFO|configuration_utils.py:668] 2023-04-03 11:44:35,944 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n","[INFO|configuration_utils.py:720] 2023-04-03 11:44:35,945 >> Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.27.0\",\n","  \"vocab_size\": 30522\n","}\n","\n","Downloading pytorch_model.bin: 100% 268M/268M [00:00<00:00, 360MB/s]\n","[INFO|modeling_utils.py:2403] 2023-04-03 11:45:11,398 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n","[INFO|modeling_distilbert.py:825] 2023-04-03 11:45:11,674 >> COME HERE YTONG !!!\n","[WARNING|modeling_utils.py:3022] 2023-04-03 11:45:12,400 >> Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3034] 2023-04-03 11:45:12,400 >> Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/20 [00:00<?, ? examples/s]04/03/2023 11:45:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-95ca0bcbaa426b3b.arrow\n","Running tokenizer on validation dataset:   0% 0/10 [00:00<?, ? examples/s]04/03/2023 11:45:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-18a9b2e0d727bdd2.arrow\n","Downloading builder script: 100% 6.47k/6.47k [00:00<00:00, 5.33MB/s]\n","Downloading extra modules: 100% 11.3k/11.3k [00:00<00:00, 9.16MB/s]\n","/content/drive/My Drive/git/transformers/src/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1740] 2023-04-03 11:45:20,651 >> ***** Running training *****\n","[INFO|trainer.py:1741] 2023-04-03 11:45:20,652 >>   Num examples = 20\n","[INFO|trainer.py:1742] 2023-04-03 11:45:20,652 >>   Num Epochs = 2\n","[INFO|trainer.py:1743] 2023-04-03 11:45:20,652 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1744] 2023-04-03 11:45:20,652 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1745] 2023-04-03 11:45:20,652 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1746] 2023-04-03 11:45:20,652 >>   Total optimization steps = 4\n","[INFO|trainer.py:1747] 2023-04-03 11:45:20,652 >>   Number of trainable parameters = 66364418\n","100% 4/4 [00:04<00:00,  1.49it/s][INFO|trainer.py:2012] 2023-04-03 11:45:25,793 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 5.1562, 'train_samples_per_second': 7.758, 'train_steps_per_second': 0.776, 'train_loss': 5.846332550048828, 'epoch': 2.0}\n","100% 4/4 [00:04<00:00,  1.03s/it]\n","[INFO|trainer.py:2814] 2023-04-03 11:45:25,812 >> Saving model checkpoint to OUTPUT_DIR\n","[INFO|configuration_utils.py:457] 2023-04-03 11:45:26,484 >> Configuration saved in OUTPUT_DIR/config.json\n","[INFO|modeling_utils.py:1762] 2023-04-03 11:45:33,306 >> Model weights saved in OUTPUT_DIR/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2163] 2023-04-03 11:45:34,028 >> tokenizer config file saved in OUTPUT_DIR/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2170] 2023-04-03 11:45:34,529 >> Special tokens file saved in OUTPUT_DIR/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     5.8463\n","  train_runtime            = 0:00:05.15\n","  train_samples            =         20\n","  train_samples_per_second =      7.758\n","  train_steps_per_second   =      0.776\n","04/03/2023 11:45:38 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:738] 2023-04-03 11:45:38,667 >> The following columns in the evaluation set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `DistilBertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3068] 2023-04-03 11:45:38,670 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3070] 2023-04-03 11:45:38,670 >>   Num examples = 10\n","[INFO|trainer.py:3073] 2023-04-03 11:45:38,670 >>   Batch size = 16\n","  0% 0/1 [00:00<?, ?it/s]04/03/2023 11:45:38 - INFO - utils_qa - Post-processing 10 example predictions split into 10 features.\n","\n","100% 10/10 [00:00<00:00, 433.71it/s]\n","04/03/2023 11:45:38 - INFO - utils_qa - Saving predictions to OUTPUT_DIR/eval_predictions.json.\n","04/03/2023 11:45:39 - INFO - utils_qa - Saving nbest_preds to OUTPUT_DIR/eval_nbest_predictions.json.\n","04/03/2023 11:45:40 - INFO - utils_qa - Saving null_odds to OUTPUT_DIR/eval_null_odds.json.\n","100% 1/1 [00:02<00:00,  2.28s/it]\n","***** eval metrics *****\n","  epoch                   =        2.0\n","  eval_HasAns_exact       =        0.0\n","  eval_HasAns_f1          =     4.7863\n","  eval_HasAns_total       =          6\n","  eval_NoAns_exact        =        0.0\n","  eval_NoAns_f1           =        0.0\n","  eval_NoAns_total        =          4\n","  eval_best_exact         =       40.0\n","  eval_best_exact_thresh  =        0.0\n","  eval_best_f1            =    41.5385\n","  eval_best_f1_thresh     =        0.0\n","  eval_exact              =        0.0\n","  eval_f1                 =     2.8718\n","  eval_runtime            = 0:00:00.16\n","  eval_samples            =         10\n","  eval_samples_per_second =     61.372\n","  eval_steps_per_second   =      6.137\n","  eval_total              =         10\n","[INFO|modelcard.py:451] 2023-04-03 11:45:42,631 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'config': 'squad_v2', 'split': 'validation', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"markdown","source":["### Improved Type 1 ALBERT\n","albert-base-v2"],"metadata":{"id":"DIGhHufmBD4M"}},{"cell_type":"code","source":["OUTPUT_PATH = '/content/drive/My Drive/git/transformers/ALBERT_OUTPUT_DIR'\n","\n","TRAIN_OR_RESUME = 'TRAIN' # change to RESUME if you want to resume from checkpoint\n","if TRAIN_OR_RESUME == 'TRAIN':\n","  rmtree(OUTPUT_PATH)"],"metadata":{"id":"Zx0KUQkI9oSl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if TRAIN_OR_RESUME == 'TRAIN':\n","  !python examples/pytorch/question-answering/run_qa.py \\\n","              --model_name_or_path albert-base-v2 \\\n","              --dataset_name squad_v2 \\\n","              --do_train \\\n","              --do_eval \\\n","              --version_2_with_negative \\\n","              --learning_rate 3e-5 \\\n","              --num_train_epochs 2 \\\n","              --max_seq_length 384 \\\n","              --doc_stride 128 \\\n","              --per_device_eval_batch_size 8  \\\n","              --per_device_train_batch_size 8   \\\n","              --output_dir ALBERT_OUTPUT_DIR \\\n","              --overwrite_output_dir \\\n","              --save_strategy \"steps\" \\\n","              --save_steps 3000\n","elif TRAIN_OR_RESUME == 'RESUME':\n","  !python examples/pytorch/question-answering/run_qa.py \\\n","              --model_name_or_path albert-base-v2 \\\n","              --dataset_name squad_v2 \\\n","              --do_train \\\n","              --do_eval \\\n","              --version_2_with_negative \\\n","              --learning_rate 3e-5 \\\n","              --num_train_epochs 2 \\\n","              --max_seq_length 384 \\\n","              --doc_stride 128 \\\n","              --per_device_eval_batch_size 8  \\\n","              --per_device_train_batch_size 8   \\\n","              --output_dir ALBERT_OUTPUT_DIR \\\n","              --save_strategy \"steps\" \\\n","              --save_steps 3000"],"metadata":{"id":"TKh9dOmZBQKs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Improved Type 1 SpanBERT\n","SpanBERT/spanbert-large-cased"],"metadata":{"id":"RRa90ZOV8nni"}},{"cell_type":"code","source":["OUTPUT_PATH = '/content/drive/My Drive/git/transformers/SPANBERT_OUTPUT_DIR'\n","\n","TRAIN_OR_RESUME = 'RESUME' # change to RESUME if you want to resume from checkpoint\n","if TRAIN_OR_RESUME == 'TRAIN':\n","  if os.path.exists(OUTPUT_PATH):\n","    rmtree(OUTPUT_PATH)"],"metadata":{"id":"9qf8liCw8ssj","executionInfo":{"status":"ok","timestamp":1680686067457,"user_tz":300,"elapsed":5,"user":{"displayName":"David Tong","userId":"10645298735537320294"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["if TRAIN_OR_RESUME == 'TRAIN':\n","  !python examples/pytorch/question-answering/run_qa.py \\\n","              --model_name_or_path SpanBERT/spanbert-large-cased \\\n","              --dataset_name squad_v2 \\\n","              --do_train \\\n","              --do_eval \\\n","              --version_2_with_negative \\\n","              --learning_rate 3e-5 \\\n","              --num_train_epochs 2 \\\n","              --max_seq_length 384 \\\n","              --doc_stride 128 \\\n","              --per_device_eval_batch_size 8  \\\n","              --per_device_train_batch_size 8   \\\n","              --output_dir SPANBERT_OUTPUT_DIR \\\n","              --overwrite_output_dir \\\n","              --save_strategy \"steps\" \\\n","              --save_steps 3000\n","elif TRAIN_OR_RESUME == 'RESUME':\n","  !python examples/pytorch/question-answering/run_qa.py \\\n","              --model_name_or_path SpanBERT/spanbert-large-cased \\\n","              --dataset_name squad_v2 \\\n","              --do_train \\\n","              --do_eval \\\n","              --version_2_with_negative \\\n","              --learning_rate 3e-5 \\\n","              --num_train_epochs 2 \\\n","              --max_seq_length 384 \\\n","              --doc_stride 128 \\\n","              --per_device_eval_batch_size 8  \\\n","              --per_device_train_batch_size 8   \\\n","              --output_dir SPANBERT_OUTPUT_DIR \\\n","              --save_strategy \"steps\" \\\n","              --save_steps 3000"],"metadata":{"id":"Zrfv5y4r8xwD"},"execution_count":null,"outputs":[]}]}